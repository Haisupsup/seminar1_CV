{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327d184e-a61f-41c5-b193-c0bae880b813",
   "metadata": {},
   "source": [
    "## 정지 영상에서 물체 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a20b48-8b4e-4a31-b980-4f9cf7bf5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f=open('coco_names.txt', 'r')\n",
    "    class_names=[line.strip() for line in f.readlines()]\n",
    "#데이터셋의 부류 이름을 담고있는 coco_names.txt파일에서 부류 이름을 읽어 class_names에 저장하기.\n",
    "#strip함수 : 문자열의 시작과 끝에서 공백을 제거하고 공백 없이 동일한 문자열을 반환\n",
    "    model=cv.dnn.readNet('yolov3.weights','yolov3.cfg')\n",
    "#readNet : 전달된 framework 문자열, 또는 model과 config 파일 이름 확장자를 분석함. 내부에서 해당 프레임워크에 맞는 readNetFromXXX() 형태의 함수를 다시 호출\n",
    "    layer_names=model.getLayerNames() #네트워크의 모든 레이어 이름을 가져와서 layer_names에 넣는다\n",
    "    out_layers=[layer_names[i-1] for i in model.getUnconnectedOutLayers()] #레이어 중 출력 레이어의 인덱스를 가져와서 output_layers에 넣는다\n",
    "    \n",
    "    return model,out_layers,class_names\n",
    "\n",
    "def yolo_detect(img,yolo_model,out_layers):\n",
    "    height,width=img.shape[0],img.shape[1] #(x,y)\n",
    "    test_img=cv.dnn.blobFromImage(img,1.0/256,(448,448),(0,0,0),swapRB=True) #이미지를 blob객체로 처리한다.\n",
    "#(입력 영상, scalefactor(0~1사이 정규화), 출력 영상 크기, 입력 영상 각 채널에서 뺄 평균값, R과 B채널 바꿀지 = True면 바꿈)\n",
    "    \n",
    "    yolo_model.setInput(test_img) #blob 객체에 setInput 함수를 적용\n",
    "    output3=yolo_model.forward(out_layers) #네트워크 순방향으로 실행.\n",
    "    \n",
    "    box,conf,id=[],[],[]\t\t# 박스, 신뢰도, 부류 번호(배열로 집어넣음)\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores=vec85[5:]\n",
    "            class_id=np.argmax(scores) #scores 중에서 최대값을 색인하여 class_id에 넣는다.\n",
    "            confidence=scores[class_id] #scores 중에서 class_id에 해당하는 값을 confidence에 넣는다.\n",
    "            if confidence>0.5:\t# 신뢰도가 50% 이상인 경우만 취함\n",
    "                centerx,centery=int(vec85[0]*width),int(vec85[1]*height)\n",
    "                w,h=int(vec85[2]*width),int(vec85[3]*height)\n",
    "                x,y=int(centerx-w/2),int(centery-h/2)\n",
    "                box.append([x,y,x+w,y+h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "            \n",
    "    ind=cv.dnn.NMSBoxes(box,conf,0.5,0.4) #노이즈 제거\n",
    "    objects=[box[i]+[conf[i]]+[id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "model,out_layers,class_names=construct_yolo_v3()\t\t# YOLO 모델 생성\n",
    "colors=np.random.uniform(0,255,size=(len(class_names),3))\t# 부류마다 색깔\n",
    "\n",
    "img=cv.imread('soccer.jpg')\n",
    "if img is None: sys.exit('파일이 없습니다.')\n",
    "\n",
    "res=yolo_detect(img,model,out_layers)\t# YOLO 모델로 물체 검출\n",
    "\n",
    "for i in range(len(res)):\t\t\t# 검출된 물체를 영상에 표시\n",
    "    x1,y1,x2,y2,confidence,id=res[i]\n",
    "    text=str(class_names[id])+'%.3f'%confidence\n",
    "    cv.rectangle(img,(x1,y1),(x2,y2),colors[id],2)\n",
    "    cv.putText(img,text,(x1,y1+30),cv.FONT_HERSHEY_PLAIN,1.5,colors[id],2)\n",
    "\n",
    "cv.imshow(\"Object detection by YOLO v.3\",img)\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c71e35-e8d3-4b8e-a4f5-947e2d0c589f",
   "metadata": {},
   "source": [
    "## YOLO v3으로 비디오에서 물체 검출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5a3526-6801-4138-b883-e2868b9f0e3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "카메라 연결 실패",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 카메라 연결 실패\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnv59\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f=open('coco_names.txt', 'r')\n",
    "    class_names=[line.strip() for line in f.readlines()]\n",
    "\n",
    "    model=cv.dnn.readNet('yolov3.weights','yolov3.cfg')\n",
    "    layer_names=model.getLayerNames()\n",
    "    out_layers=[layer_names[i-1] for i in model.getUnconnectedOutLayers()]\n",
    "    \n",
    "    return model,out_layers,class_names\n",
    "\n",
    "def yolo_detect(img,yolo_model,out_layers):\n",
    "    height,width=img.shape[0],img.shape[1]\n",
    "    test_img=cv.dnn.blobFromImage(img,1.0/256,(448,448),(0,0,0),swapRB=True)\n",
    "    \n",
    "    yolo_model.setInput(test_img)\n",
    "    output3=yolo_model.forward(out_layers)\n",
    "    \n",
    "    box,conf,id=[],[],[]\t\t# 박스, 신뢰도, 부류 번호\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores=vec85[5:]\n",
    "            class_id=np.argmax(scores)\n",
    "            confidence=scores[class_id]\n",
    "            if confidence>0.5:\t# 신뢰도가 50% 이상인 경우만 취함\n",
    "                centerx,centery=int(vec85[0]*width),int(vec85[1]*height)\n",
    "                w,h=int(vec85[2]*width),int(vec85[3]*height)\n",
    "                x,y=int(centerx-w/2),int(centery-h/2)\n",
    "                box.append([x,y,x+w,y+h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "            \n",
    "    ind=cv.dnn.NMSBoxes(box,conf,0.5,0.4)\n",
    "    objects=[box[i]+[conf[i]]+[id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "model,out_layers,class_names=construct_yolo_v3()\t\t# YOLO 모델 생성\n",
    "colors=np.random.uniform(0,255,size=(len(class_names),3))\t# 부류마다 색깔\n",
    "\n",
    "cap=cv.VideoCapture(0,cv.CAP_DSHOW) #비디오 캠쳐함수 도입, \n",
    "if not cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret: sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        \n",
    "    res=yolo_detect(frame,model,out_layers)   \n",
    " \n",
    "    for i in range(len(res)):\n",
    "        x1,y1,x2,y2,confidence,id=res[i]\n",
    "        text=str(class_names[id])+'%.3f'%confidence\n",
    "        cv.rectangle(frame,(x1,y1),(x2,y2),colors[id],2)\n",
    "        cv.putText(frame,text,(x1,y1+30),cv.FONT_HERSHEY_PLAIN,1.5,colors[id],2)\n",
    "    \n",
    "    cv.imshow(\"Object detection from video by YOLO v.3\",frame)\n",
    "    \n",
    "    key=cv.waitKey(1) \n",
    "    if key==ord('q'): break \n",
    "    \n",
    "cap.release()\t\t# 카메라와 연결을 끊음\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67115e73-a5be-4031-8ec0-81f6b69eb7c6",
   "metadata": {},
   "source": [
    "## YOLO v3의 비디오 처리량 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414861c2-f1ac-4e52-a0fe-46b7ddc5f249",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "카메라 연결 실패",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 카메라 연결 실패\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f=open('coco_names.txt', 'r')\n",
    "    class_names=[line.strip() for line in f.readlines()]\n",
    "\n",
    "    model=cv.dnn.readNet('yolov3.weights','yolov3.cfg')\n",
    "    layer_names=model.getLayerNames()\n",
    "    out_layers=[layer_names[i-1] for i in model.getUnconnectedOutLayers()]\n",
    "    \n",
    "    return model,out_layers,class_names\n",
    "\n",
    "def yolo_detect(img,yolo_model,out_layers):\n",
    "    height,width=img.shape[0],img.shape[1]\n",
    "    test_img=cv.dnn.blobFromImage(img,1.0/256,(448,448),(0,0,0),swapRB=True)\n",
    "    \n",
    "    yolo_model.setInput(test_img)\n",
    "    output3=yolo_model.forward(out_layers)\n",
    "    \n",
    "    box,conf,id=[],[],[]\t\t# 박스, 신뢰도, 부류 번호\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores=vec85[5:]\n",
    "            class_id=np.argmax(scores)\n",
    "            confidence=scores[class_id]\n",
    "            if confidence>0.5:\t# 신뢰도가 50% 이상인 경우만 취함\n",
    "                centerx,centery=int(vec85[0]*width),int(vec85[1]*height)\n",
    "                w,h=int(vec85[2]*width),int(vec85[3]*height)\n",
    "                x,y=int(centerx-w/2),int(centery-h/2)\n",
    "                box.append([x,y,x+w,y+h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "            \n",
    "    ind=cv.dnn.NMSBoxes(box,conf,0.5,0.4)\n",
    "    objects=[box[i]+[conf[i]]+[id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "model,out_layers,class_names=construct_yolo_v3()\t\t# YOLO 모델 생성\n",
    "colors=np.random.uniform(0,255,size=(len(class_names),3))\t# 부류마다 색깔\n",
    "\n",
    "cap=cv.VideoCapture(0,cv.CAP_DSHOW)\n",
    "if not cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "\n",
    "import time\n",
    "#비디오 처리량을 측정하기 위해 불러오기\n",
    "start=time.time()\n",
    "#시작 시간을 start에 자장하기\n",
    "n_frame=0\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret: sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        \n",
    "    res=yolo_detect(frame,model,out_layers)   \n",
    " \n",
    "    for i in range(len(res)):\n",
    "        x1,y1,x2,y2,confidence,id=res[i]\n",
    "        text=str(class_names[id])+'%.3f'%confidence\n",
    "        cv.rectangle(frame,(x1,y1),(x2,y2),colors[id],2)\n",
    "        cv.putText(frame,text,(x1,y1+30),cv.FONT_HERSHEY_PLAIN,1.5,colors[id],2)\n",
    "    \n",
    "    cv.imshow(\"Object detection from video by YOLO v.3\",frame)\n",
    "    n_frame+=1\n",
    "#처리한 프레임 수를 1만큼 증가한다.    \n",
    "    key=cv.waitKey(1) \n",
    "    if key==ord('q'): break \n",
    "#끝난 시간을 end에 저장하고, 69행은 비디오 처리량을 출력한다.\n",
    "end=time.time()\n",
    "print('처리한 프레임 수=',n_frame,', 경과 시간=',end-start,'\\n초당 프레임 수=',n_frame/(end-start))\n",
    "\n",
    "cap.release()\t\t# 카메라와 연결을 끊음\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91cc29-d180-4c5c-9f72-608e31e12206",
   "metadata": {},
   "source": [
    "## oxford pets 데이터셋으로 U-net 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54f56670-f9d0-44e2-b61d-9234c2c2e388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "207/207 [==============================] - ETA: 0s - batch: 103.0000 - size: 32.0000 - loss: 0.8054 - accuracy: 0.7189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnv59\\AppData\\Local\\anaconda3\\envs\\open_env\\lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "C:\\Users\\tnv59\\AppData\\Local\\anaconda3\\envs\\open_env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 532s 3s/step - batch: 103.0000 - size: 32.0000 - loss: 0.8054 - accuracy: 0.7189 - val_loss: 3.0310 - val_accuracy: 0.5854\n",
      "Epoch 2/10\n",
      "207/207 [==============================] - 461s 2s/step - batch: 103.0000 - size: 32.0000 - loss: 0.5326 - accuracy: 0.7859 - val_loss: 2.7959 - val_accuracy: 0.5854\n",
      "Epoch 3/10\n",
      "207/207 [==============================] - 454s 2s/step - batch: 103.0000 - size: 32.0000 - loss: 0.4610 - accuracy: 0.8165 - val_loss: 0.5214 - val_accuracy: 0.7960\n",
      "Epoch 4/10\n",
      "207/207 [==============================] - 486s 2s/step - batch: 103.0000 - size: 32.0000 - loss: 0.4129 - accuracy: 0.8359 - val_loss: 0.4399 - val_accuracy: 0.8268\n",
      "Epoch 5/10\n",
      "207/207 [==============================] - 493s 2s/step - batch: 103.0000 - size: 32.0000 - loss: 0.3823 - accuracy: 0.8483 - val_loss: 0.4209 - val_accuracy: 0.8338\n",
      "Epoch 6/10\n",
      "207/207 [==============================] - 492s 2s/step - batch: 103.0000 - size: 32.0000 - loss: 0.3525 - accuracy: 0.8605 - val_loss: 0.4225 - val_accuracy: 0.8333\n",
      "Epoch 7/10\n",
      "207/207 [==============================] - 461s 2s/step - batch: 103.0000 - size: 32.0000 - loss: 0.3278 - accuracy: 0.8700 - val_loss: 0.4793 - val_accuracy: 0.8132\n",
      "Epoch 8/10\n",
      "207/207 [==============================] - 495s 2s/step - batch: 103.0000 - size: 32.0000 - loss: 0.3059 - accuracy: 0.8780 - val_loss: 0.4066 - val_accuracy: 0.8425\n",
      "Epoch 9/10\n",
      "207/207 [==============================] - 462s 2s/step - batch: 103.0000 - size: 32.0000 - loss: 0.2852 - accuracy: 0.8860 - val_loss: 0.4337 - val_accuracy: 0.8389\n",
      "Epoch 10/10\n",
      "207/207 [==============================] - 483s 2s/step - batch: 103.0000 - size: 32.0000 - loss: 0.2695 - accuracy: 0.8919 - val_loss: 0.4400 - val_accuracy: 0.8406\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import random\n",
    "import cv2 as cv\n",
    "\n",
    "input_dir='C:/Users/tnv59/open_env/img/images/images'\n",
    "target_dir='C:/Users/tnv59/open_env/img/annotations/annotations/trimaps'\n",
    "img_siz=(160,160)\t# 모델에 입력되는 영상 크기\n",
    "n_class=3\t\t# 분할 레이블 (1:물체, 2:배경, 3:경계)\n",
    "batch_siz=32\t\t# 미니 배치 크기\n",
    "\n",
    "img_paths=sorted([os.path.join(input_dir,f) for f in os.listdir(input_dir) if f.endswith('.jpg')])\n",
    "label_paths=sorted([os.path.join(target_dir,f) for f in os.listdir(target_dir) if f.endswith('.png') and not f.startswith('.')])\n",
    "\n",
    "class OxfordPets(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size,img_size,img_paths,label_paths):\n",
    "        self.batch_size=batch_size\n",
    "        self.img_size=img_size\n",
    "        self.img_paths=img_paths\n",
    "        self.label_paths=label_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_paths)//self.batch_size\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        i=idx*self.batch_size\n",
    "        batch_img_paths=self.img_paths[i:i+self.batch_size]\n",
    "        batch_label_paths=self.label_paths[i:i+self.batch_size]\n",
    "        x=np.zeros((self.batch_size,)+self.img_size+(3,),dtype=\"float32\")\n",
    "        for j,path in enumerate(batch_img_paths):\n",
    "            img=load_img(path,target_size=self.img_size)\n",
    "            x[j]=img\n",
    "        y=np.zeros((self.batch_size,)+self.img_size+(1,),dtype=\"uint8\")\n",
    "        for j,path in enumerate(batch_label_paths):\n",
    "            img=load_img(path,target_size=self.img_size,color_mode=\"grayscale\")\n",
    "            y[j]=np.expand_dims(img,2)\n",
    "            y[j]-=1\t\t# 부류 번호를 1,2,3에서 0,1,2로 변환\n",
    "        return x,y\n",
    "\n",
    "def make_model(img_size,num_classes):\n",
    "    inputs=keras.Input(shape=img_size+(3,))\n",
    "\n",
    "    # U-net의 다운 샘플링(축소 경로)\n",
    "    x=layers.Conv2D(32,3,strides=2,padding='same')(inputs)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    x=layers.Activation('relu')(x)\n",
    "    previous_block_activation=x\t\t# 지름길 연결을 위해\n",
    "\n",
    "    for filters in [64,128,256]:\n",
    "        x=layers.Activation('relu')(x)\n",
    "        x=layers.SeparableConv2D(filters,3,padding='same')(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "        x=layers.Activation('relu')(x)\n",
    "        x=layers.SeparableConv2D(filters,3,padding='same')(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "        x=layers.MaxPooling2D(3,strides=2,padding='same')(x)\n",
    "        residual=layers.Conv2D(filters,1,strides=2,padding='same')(previous_block_activation)\n",
    "        x=layers.add([x,residual])\t# 지름길 연결  \n",
    "        previous_block_activation=x\t# 지름길 연결을 위해\n",
    "\n",
    "    # U-net의 업 샘플링(확대 경로)\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x=layers.Activation('relu')(x)\n",
    "        x=layers.Conv2DTranspose(filters,3,padding='same')(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "        x=layers.Activation('relu')(x)\n",
    "        x=layers.Conv2DTranspose(filters,3,padding='same')(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "        x=layers.UpSampling2D(2)(x)\n",
    "        residual=layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual=layers.Conv2D(filters,1,padding='same')(residual)\n",
    "        x=layers.add([x,residual])\t# 지름길 연결\n",
    "        previous_block_activation=x\t# 지름길 연결을 위해\n",
    "\n",
    "    outputs=layers.Conv2D(num_classes,3,activation='softmax',padding='same')(x)\n",
    "    model=keras.Model(inputs, outputs)\t# 모델 생성\n",
    "    return model\n",
    "\n",
    "model=make_model(img_siz,n_class)\t\t# 모델 생성\n",
    "\n",
    "random.Random(1).shuffle(img_paths)\n",
    "random.Random(1).shuffle(label_paths)\n",
    "test_samples=int(len(img_paths)*0.1)\t# 10%를 테스트 집합으로 사용\n",
    "train_img_paths=img_paths[:-test_samples]\n",
    "train_label_paths=label_paths[:-test_samples]\n",
    "test_img_paths=img_paths[-test_samples:]\n",
    "test_label_paths=label_paths[-test_samples:]\n",
    "\n",
    "train_gen=OxfordPets(batch_siz,img_siz,train_img_paths,train_label_paths) # 훈련 집합\n",
    "test_gen=OxfordPets(batch_siz,img_siz,test_img_paths,test_label_paths) # 검증 집합\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "cb=[keras.callbacks.ModelCheckpoint('oxford_seg.h5',save_best_only=True)] # 학습 결과 자동 저장\n",
    "model.fit(train_gen,epochs=10,validation_data=test_gen,callbacks=cb)\n",
    "\n",
    "preds=model.predict(test_gen)\t# 예측\n",
    "\n",
    "cv.imshow('Sample image',cv.imread(test_img_paths[0]))# 0번 영상 디스플레이\n",
    "cv.imshow('Segmentation label',cv.imread(test_label_paths[0])*64)\n",
    "cv.imshow('Segmentation prediction',preds[0]) # 0번 영상 예측 결과 디스플레이\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8997cb-eada-4409-a874-374e96754b89",
   "metadata": {},
   "source": [
    "## pixellib 라이브러리로 정지 영상을 의미 분할하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40852093-d794-47d0-b8a6-295457dd97de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tnv59\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\tnv59\\appdata\\roaming\\python\\python39\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\tnv59\\appdata\\roaming\\python\\python39\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.16.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\tnv59\\appdata\\local\\anaconda3\\envs\\open_env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras, absl-py\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.6.0\n",
      "    Uninstalling keras-2.6.0:\n",
      "      Successfully uninstalled keras-2.6.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.15.0\n",
      "    Uninstalling absl-py-0.15.0:\n",
      "      Successfully uninstalled absl-py-0.15.0\n",
      "Successfully installed absl-py-2.1.0 keras-2.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8e97b-e84e-4a44-becc-a8e5c05ee5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b895425b-87e7-402f-ad04-d58a38e6541a",
   "metadata": {},
   "source": [
    "## pixellib 라이브러리로 비디오를 의미 분할하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a2151-7939-4568-97ac-c941a4bc68b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb0861-699a-4bdf-ad26-1dece9ced2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac5556-f0f6-4b17-944b-ad96cdd8b0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbe28d-eb05-42e2-bd27-8700adadbc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0495e93-a4ec-468b-aa98-b9e855ac249c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267cad3-45f6-4fab-83e5-95da6ca0930c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbc87a-ddae-4f62-b1e2-615719d71fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cccef2-7f00-49a2-a761-4f7febd2b4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05668151-033f-4af7-a583-c7179aada644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979605eb-0f7c-443b-9029-2a5cbb14492a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0a559-7936-4ab5-aa42-28769b522bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948e19a-97e7-4fa0-9aff-faf1808c23ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3a47a-d0dd-4d9d-8327-8d262d7bb746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_practice",
   "language": "python",
   "name": "opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
